{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验3：LeNet-5 网络的构建\n",
    "## 一、实验介绍\n",
    "以 MNIST 数据集为对象，利用 Pytorch进行 LeNet-5 模型设计、数据加载、损\n",
    "失函数及优化器定义，评估模型的性能。\n",
    "\n",
    "通过之前几节，我们学习了构建一个完整卷积神经网络的所需组件。\n",
    "回想一下，之前我们将softmax回归模型（`sec_softmax_scratch`）和多层感知机模型（`sec_mlp_scratch`）应用于Fashion-MNIST数据集中的服装图片。\n",
    "为了能够应用softmax回归和多层感知机，我们首先将每个大小为$28\\times28$的图像展平为一个784维的固定长度的一维向量，然后用全连接层对其进行处理。\n",
    "- 而现在，我们已经掌握了卷积层的处理方法，我们可以在图像中保留空间结构。\n",
    "- 同时，用卷积层代替全连接层的另一个好处是：模型更简洁、所需的参数更少。\n",
    "\n",
    "本节将介绍LeNet，它是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注。\n",
    "这个模型是由AT&T贝尔实验室的研究员Yann LeCun在1989年提出的（并以其命名），目的是识别图像`LeCun.Bottou.Bengio.ea.1998`中的手写数字。\n",
    "当时，Yann LeCun发表了第一篇通过反向传播成功训练卷积神经网络的研究，这项工作代表了十多年来神经网络研究开发的成果。\n",
    "\n",
    "当时，LeNet取得了与支持向量机（support vector machines）性能相媲美的成果，成为监督学习的主流方法。\n",
    "LeNet被广泛用于自动取款机（ATM）机中，帮助识别处理支票的数字。\n",
    "时至今日，一些自动取款机仍在运行Yann LeCun和他的同事Leon Bottou在上世纪90年代写的代码呢！\n",
    "\n",
    "## 二、实验要求\n",
    "理解常用 Pytorch实现卷积神经网络的流程，定义数据加载器、损失函数和优化器，构建完整的训练流程。\n",
    "## 三、实验内容\n",
    "\n",
    "### 3.1 导入所需的依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "import torchvision\n",
    "\n",
    "import torch.utils.data \n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 数据预处理\n",
    "在概率论的角度来看，Normalize 里的 mean 和 std 分别对应“期望”（Arithmetic Mean / Expectation）和“标准差”（Standard Deviation）。这两者是分布最常见的两个刻画参数：  \n",
    "- mean（期望）描述了数据分布的集中趋势；  \n",
    "- std（标准差）则描述了分布的离散程度。  \n",
    "\n",
    "至于为什么在 PyTorch / torchvision 中要传递元组（tuple）而不是单个数值，这是因为在图像处理时通常有多个通道（例如 RGB 图像有 3 个通道），需要对每个通道各自进行正则化：  \n",
    "$$\n",
    "\\text{output}[c] = \\frac{\\text{input}[c] - \\text{mean}[c]}{\\text{std}[c]}\n",
    "$$\n",
    "因此，mean 和 std 都需要以元组的形式提供多个通道对应的参数（如：(mean_R, mean_G, mean_B), (std_R, std_G, std_B)）。如果图像只有单通道，也可以只传递包含一个元素的元组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data.dataloader\n",
    "import torchvision.transforms\n",
    "\n",
    "transform_calling_function = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.13, ), (0.30, ))\n",
    "])\n",
    "\n",
    "\n",
    "dataset_train = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_calling_function\n",
    ")\n",
    "\n",
    "dataset_test = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_calling_function\n",
    ")\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset_train,\n",
    "    shuffle=False,\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset_test,\n",
    "    shuffle=False,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 定义LeNet类\n",
    "\n",
    "总体来看，(**LeNet（LeNet-5）由两个部分组成：**)(卷积编码器和全连接层密集块)\n",
    "\n",
    "* 卷积编码器：由两个卷积层组成;\n",
    "* 全连接层密集块：由三个全连接层组成。\n",
    "\n",
    "该架构如下所示。\n",
    "\n",
    "![LeNet中的数据流。输入是手写数字，输出为10种可能结果的概率。](../../pytorch/img/lenet.svg)\n",
    "\n",
    "每个卷积块中的基本单元是一个卷积层、一个sigmoid激活函数和平均汇聚层。请注意，虽然ReLU和最大汇聚层更有效，但它们在20世纪90年代还没有出现。每个卷积层使用$5\\times 5$卷积核和一个sigmoid激活函数。这些层将输入映射到多个二维特征输出，通常同时增加通道的数量。第一卷积层有6个输出通道，而第二个卷积层有16个输出通道。每个$2\\times2$池操作（步幅2）通过空间下采样将维数减少4倍。卷积的输出形状由批量大小、通道数、高度、宽度决定。\n",
    "\n",
    "为了将卷积块的输出传递给稠密块，我们必须在小批量中展平每个样本。换言之，我们将这个四维输入转换成全连接层所期望的二维输入。这里的二维表示的第一个维度索引小批量中的样本，第二个维度给出每个样本的平面向量表示。LeNet的稠密块有三个全连接层，分别有120、84和10个输出。因为我们在执行分类任务，所以输出层的10维对应于最后输出结果的数量。\n",
    "\n",
    "通过下面的LeNet代码，可以看出用深度学习框架实现此类模型非常简单。我们只需要实例化一个`Sequential`块并将需要的层连接在一起。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**输入图像**：\n",
    "- 经典的输入尺寸是 `(1, 32, 32)` 单通道灰度图像 (MNIST)，但你也可以适当调整。\n",
    "\n",
    "**结构顺序**：\n",
    "\n",
    "1. **卷积层（Conv1）**：\n",
    "   - 输入通道：1\n",
    "   - 输出通道：6\n",
    "   - 卷积核尺寸：5×5\n",
    "   - 激活函数：Sigmoid（经典LeNet中为sigmoid，现通常用ReLU）\n",
    "\n",
    "2. **平均池化层（AvgPool1）**：\n",
    "   - 核尺寸：2×2\n",
    "   - 步长：2\n",
    "\n",
    "3. **卷积层（Conv2）**：\n",
    "   - 输入通道：6\n",
    "   - 输出通道：16\n",
    "   - 卷积核尺寸：5×5\n",
    "   - 激活函数：Sigmoid（经典）或ReLU\n",
    "\n",
    "4. **平均池化层（AvgPool2）**：\n",
    "   - 核尺寸：2×2\n",
    "   - 步长：2\n",
    "\n",
    "5. **Flatten 展平**：\n",
    "   - 把特征图展平成一维向量。\n",
    "\n",
    "6. **全连接层（FC1）**：\n",
    "   - 输入维度：400（若输入32×32图像，经上述层变换最终变成16通道×5×5尺寸=400个神经元）\n",
    "   - 输出维度：120\n",
    "   - 激活函数：Sigmoid（经典）或ReLU\n",
    "\n",
    "7. **全连接层（FC2）**：\n",
    "   - 输入维度：120\n",
    "   - 输出维度：84\n",
    "   - 激活函数：Sigmoid（经典）或ReLU\n",
    "\n",
    "8. **全连接输出层（FC3）**：\n",
    "   - 输入维度：84\n",
    "   - 输出维度：10（用于MNIST的10分类）\n",
    "   - 激活函数：一般不加，直接输出Logits\n",
    "\n",
    "**常见激活函数选择说明**：\n",
    "- **经典论文版本（LeNet-5）**：使用Sigmoid激活函数\n",
    "- **现代常用实践**：使用ReLU替代Sigmoid以提高训练效率\n",
    "\n",
    "**整体网络结构示意图**：\n",
    "\n",
    "```\n",
    "Input → Conv(1→6, 5×5) → Pool(2×2) → Conv(6→16, 5×5) → Pool(2×2) → Flatten → FC(400→120) → FC(120→84) → FC(84→10) → Output\n",
    "```\n",
    "\n",
    "请根据以上提示信息，在不参考任何代码示例的情况下，默写PyTorch代码实现LeNet。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "class My_LeNet(torch.nn.Module): # 继承\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # python2的写法：\n",
    "        # super(My_LeNet, self).__init__(*args, **kwargs) # 初始化父类，super是父类（超类）的意思\n",
    "        # python3的写法：\n",
    "        super().__init__(*args, **kwargs) # 初始化父类，super是父类（超类）的意思\n",
    "        self.convolution_layer1 = torch.nn.Conv2d(\n",
    "            in_channels=1, \n",
    "            out_channels=6, # 一共六层参数\n",
    "            kernel_size=(5, 5), \n",
    "            padding=2 # padding成32*32 \n",
    "        )\n",
    "        self.avg_pooling1 = torch.nn.AvgPool2d( # 缩成 16*16 还是六层参数\n",
    "            kernel_size=(2, 2), \n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\n",
    "        self.convolution_layer2 = torch.nn.Conv2d(\n",
    "            in_channels=6, \n",
    "            out_channels=16, # 我有16个卷积核，每个卷积核都会作用在6个输入平面上，最后输出16层\n",
    "            kernel_size=(5, 5)\n",
    "        ) # 输出尺寸：（16-5+1）= 12 * 12 * 16\n",
    "\n",
    "        self.avg_pooling2 = torch.nn.AvgPool2d(\n",
    "            kernel_size=(2, 2), \n",
    "            stride=2\n",
    "        ) # 输出尺寸：6 * 6 * 16\n",
    "\n",
    "        # ============== 重点：自动推导in_features ===============\n",
    "        with torch.no_grad():\n",
    "            fake_input = torch.zeros((1, 1, 28, 28))  # 假设输入图片为(1, 1, 32, 32)\n",
    "            output = self.convolution_layer1(fake_input)\n",
    "            output = self.avg_pooling1(output)\n",
    "            output = self.convolution_layer2(output)\n",
    "            output = self.avg_pooling2(output)\n",
    "            output_dim = output.view(1, -1).shape[1]  # 自动推导尺寸\n",
    "\n",
    "        # ============== 推导完成 ===========================\n",
    "\n",
    "        self.fully_connected_layer1 = torch.nn.Linear(in_features=output_dim, out_features=120, bias=True)\n",
    "        self.fully_connected_layer2 = torch.nn.Linear(in_features=120, out_features=84, bias=True)\n",
    "        self.fully_connected_layer3 = torch.nn.Linear(in_features=84, out_features=10, bias=True)\n",
    "        \n",
    "        # 这里只创建了一个激活函数，因为激活函数不用训练。。。\n",
    "        self.activation_func = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.convolution_layer1(input)\n",
    "        output = self.activation_func(output)\n",
    "        output = self.avg_pooling1(output)\n",
    "\n",
    "        output = self.convolution_layer2(output)\n",
    "        output = self.activation_func(output)\n",
    "        output = self.avg_pooling2(output)\n",
    "\n",
    "        output = output.view(output.size(0), -1) # 展平\n",
    "        # print(\"Flattened shape:\", output.shape) # 确认一下维度，是否与预期一致\n",
    "        \n",
    "        output = self.fully_connected_layer1(output)\n",
    "        output = self.activation_func(output)\n",
    "        output = self.fully_connected_layer2(output)\n",
    "        output = self.activation_func(output)\n",
    "        output = self.fully_connected_layer3(output)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 初始化模型、损失函数和优化器\n",
    "\n",
    "其实交叉熵损失可以传入权重，让少数类设置更大权重，重要类别设置更大权重\n",
    "样本量不平衡时的损失调整\n",
    "例如：在100张图片中\n",
    "\n",
    "- 类别0: 50张 -> weight=1.0\n",
    "- 类别1: 25张 -> weight=2.0\n",
    "- 类别2: 25张 -> weight=2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"training on {device.type}\")\n",
    "\n",
    "model = My_LeNet().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "尊嘟假嘟\n",
      "Epoch[0/10], loss:0.28058180287480355\n",
      "Epoch[1/10], loss:0.27254513079921405\n",
      "Epoch[2/10], loss:0.2648617774248123\n",
      "Epoch[3/10], loss:0.2575187755127748\n",
      "Epoch[4/10], loss:0.25051901638507845\n",
      "Epoch[5/10], loss:0.24378356399635473\n",
      "Epoch[6/10], loss:0.23733534601827463\n",
      "Epoch[7/10], loss:0.2311368878930807\n",
      "Epoch[8/10], loss:0.22521438238521416\n",
      "Epoch[9/10], loss:0.21951047368347645\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class O:\n",
    "    def o():\n",
    "        print(\"尊嘟假嘟\")\n",
    "\n",
    "O.o()\n",
    "\n",
    "import os\n",
    "\n",
    "# 定义保存路径\n",
    "path_to_saved_parameters = \"./parameters\"\n",
    "\n",
    "# 创建保存目录\n",
    "if not os.path.exists(path_to_saved_parameters):\n",
    "    os.makedirs(path_to_saved_parameters)\n",
    "\n",
    "\n",
    "\n",
    "def train(loader = train_data_loader, \n",
    "          epochs = 0xA):\n",
    "    model.train() # Set the module in training mode\n",
    "    for current_epoch in range(epochs):\n",
    "        # 统计一个epoch的总loss，最后除以训练集的size\n",
    "        epoch_loss = 0\n",
    "    \n",
    "        for img, true_label in loader:\n",
    "            # Move input data to the same device as model\n",
    "            img = img.to(device)\n",
    "            true_label = true_label.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() # 优化器里保存了上次的loss的梯度\n",
    "            output_label = model.forward(img)\n",
    "            loss = criterion(output_label, true_label)\n",
    "            loss.backward() # 计算完loss对每个参数的梯度后，将梯度存储到参数的.grad属性里\n",
    "            optimizer.step() # 再由优化器进行优化\n",
    "            epoch_loss += loss.item()\n",
    "        # 保存参数\n",
    "        torch.save({\n",
    "            'epoch': current_epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, f\"{path_to_saved_parameters}/checkpoint_epoch_{current_epoch}.pt\")\n",
    "\n",
    "        print(f\"Epoch[{current_epoch}/{epochs}], loss:{epoch_loss/len(loader)}\")\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\25891\\AppData\\Local\\Temp\\ipykernel_33964\\1014593513.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(load_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  93.26%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model = model, \n",
    "             loader = test_data_loader,\n",
    "             load_path = \"./parameters/checkpoint_epoch_5.pt\"):\n",
    "    model.eval()\n",
    "\n",
    "    # 加载模型参数\n",
    "    checkpoint = torch.load(load_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy: .2f}%\")\n",
    "\n",
    "evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of My_LeNet(\n",
       "  (convolution_layer1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (avg_pooling1): AvgPool2d(kernel_size=(2, 2), stride=2, padding=0)\n",
       "  (convolution_layer2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (avg_pooling2): AvgPool2d(kernel_size=(2, 2), stride=2, padding=0)\n",
       "  (fully_connected_layer1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fully_connected_layer2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fully_connected_layer3): Linear(in_features=84, out_features=10, bias=True)\n",
       "  (activation_func): ReLU()\n",
       ")>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
